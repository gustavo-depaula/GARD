\chapter{Conclusion}

To address the challenges of data scarcity in the field of runway detection and segmentation,
this paper introduced a novel, open-source, data augmentation technique based on a multi-step
Stable Diffusion pipeline, called Canny2Concrete.

Canny2Concrete extracts features from existing datasets and outputs images
that retain a similar structure, especially the runway shape, position, and markings, but are
customizable in respect to scenery, weather, and lighting conditions, guided by
a text prompts. The images generated by the pipeline are already
labeled, saving hours of handcraft labelling.

The pipeline is tested by augmenting real runway images from the LARD \cite{ducoffe_lard_2023}
dataset. Three datasets are generated, the Base Images dataset,
containing 6498 images, the Variant Images dataset, containing 19494 images, and
the Variant Images With Occlusion dataset, containing 19494 images. In total,
45486 images, with a diverse range of weather, lighting, background, and runway
occlusion conditions and effects were generated and are publicly on Kaggle.

All the images have along with
them a JSON file
with metadata allowing any researcher to replicate that exact image, TXT label
ready to be used in training YOLO models, and a simple binary segmentation mask
image. To the best of my knowledge, this is the largest synthetic runway
dataset publicly available.

Experimental evaluation was done training state-of-the-art detection and
segmentation models with both the new datasets and a benchmark dataset, and
these models were evaluated on a real-world dataset. The results demonstrate
the effectiveness of the Canny2Concrete pipeline in generating realistic runway
images, and the viability of diffusion-based augmentation for runway segmentation
tasks.

\section{Further Work}

The biggest limitation in the Cany2Concrete pipeline is the need for template
images. This limits the variety of structures that can be generated, and require
a manual selection of images. Future work could be done in either
programatically generating canny edge images without extracting them from any
given image. It would also help if the diffusion model understood better the
concept of a aerial view of a runway, so that it wouldn't need detailed canny
edges to generate a realistic image.

Another limitation is that, due to the chosen Stable Diffusion model, the
images don't look as real photos, but more like digital art. Future work could
shrink "Sim2Real" gap. This could be done by using a different diffusion model,
or by adding another module to the pipeline that would convert images to this
more realistic style.

The pipeline could also generate better results by improving the outpainting
techniques used in the variant image generation module. The current techniques
generate noticeable borders between the original image and the outpainted
background.


