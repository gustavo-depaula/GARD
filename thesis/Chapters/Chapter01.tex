%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************
Aviation is regarded as highly safe medium of transportation, and is marked by high degrees of automation, reducing the biggest cause (85\% of the general aviation crashes) of accidents: pilot error \cite{li_factors_2001}. Increased automation reduces cognitive, fatigue, and inexperience risks for pilots and is an important factor, alongside better training and safety regulations, for the rapidly decrease (1959-2024) in fatal accidents rate \cite{airbus_fatal_nodate}.

Although the approach and landing phases are the minority of the flight time, they contribute disproportionately to accidents, corroborated by major commercial airplane companies Boeing (7\% in approach, 36\% in landing) \cite{boeing_statistical_2024}, and Airbus (59\% in landing, 11\% in approach) \cite{airbus_accidents_nodate}.

Contributing as a risk-factor, the approach and landing phases are flight phases requiring human intervention. This has led to an increased interest in the development of autonomous landing (autoland) systems, which autonomously navigate civil aircraft or UAVs (e.g. drones) during landing. Currently, most autoland systems are based on radio signals that provide guidance to the system, such as ILS (instrument landing system) and PAR (precision approach radar).

Radio-based autonomous landing systems allow landing in extremely adverse weather conditions and low visibility, but they have a high-cost of deployment and maintenance, can suffer from electromagnetic and radio interference, and require on-the-ground specialized equipment to support the aircraft (e.g. localizer and glideslope).

The recent advancements in the Computer Vision field sparked interest \cite{airbus_airbus_2021} in developing vision-based autoland systems, which use visual navigation to guide the aircraft during the approach and landing phases. In \cite{xin_vision-based_2022}, the authors describe key advantages of vision-based autoland systems for UAV: autonomy, low cost, resistance to interference, and ability to be combined with other navigation methods for higher accuracy. Vision based landing is especially attractive for drones often landing in extreme military, environmental, or disaster relief situations, where runways may not have the necessary equipment for radio-based systems.

Two key parts of an autonomous landing system is detection and segmentation of runways, the former detects and locates the runway by drawing a bounding box, and the latter, more granularly, works at the pixel-level, identifying the object's exact shape.

Detection and segmentation methods can be divided into two fields: traditional methods and deep-learning based methods. Traditional methods employ handcrafted features and mathematical rules to segment images, e.g. texture, line, and shape features of runways \cite{aytekin_texture-based_2013} \cite{ye_research_2020}. They tend to be faster to run and not require training, but because they require handcrafted rules, they often fail to generalize to out-of-sample images and complex real-world scenarios that involve similar objects like roads and multiple runways and adverse weather conditions.

Compared to traditional methods, deep-learning based ones generalize and perform better on out-of-sample images, with the latest published runway segmentation papers all having used this approach and getting better results than traditional methods \cite{chen_image-based_2024} \cite{wang_valnet_2024}.

However, because of their data-hungry nature, previous research \cite{wang_valnet_2024} \cite{chen_image-based_2024} \cite{chen_bars_2023} \cite{ducoffe_lard_2023} all note the lack of publicly available, high-quality, large, real-world datasets of aeronautical images for runway segmentation. To close this gap, the researchers have built and published datasets of synthetic images from flight simulators such as X-Plane \cite{chen_bars_2023} \cite{wang_valnet_2024}, Microsoft Flight Simulator \cite{chen_image-based_2024}, and images collected from Google Earth Studio \cite{ducoffe_lard_2023}.

While these synthetic datasets have contributed to advancements in the field, there remains an opportunity for an open-source, realistic, high-quality dataset that has runway image data covering more varied weather, lighting, environments, and structures, and, even better, an open-source image generator that is capable of generating novel images or augmenting an existing dataset.

In recent years, striking advancements were made in the field of image generation models, with many open- and closed-source models being published for general use, such as DALL-E \cite{betker_improving_nodate}, Midjourney \cite{midjourney_midjourney_nodate}, Kandinsky \cite{arkhipkin_kandinsky_2024}, and Stable Diffusion \cite{rombach_high-resolution_2022}. These recently state-of-the-art models are latent diffusion models (LDM), which outperform previous GAN-based techniques. These image-generation models are now so capable of generating realistic images that researchers are studying the impacts of generative AI on the spread of fake news \cite{loth_blessing_2024}.

These recent research advancements opened new opportunities for synthetic data to close the gap on the lack of real images in object detection and segmentation tasks. This idea has progressed in areas such as medical images \cite{saragih_using_2024}, urban applications \cite{reutov_generating_2023}, and apple detection in orchards \cite{voetman_big_2023}. But up to my knowledge of public research, there has been no generative AI-based open-source dataset for runway detection and segmentation.

To address the challenges of lack of data in the context of runway segmentation tasks, this paper introduces a novel, open-source, data augmentation technique based on a multi-step Stable Diffusion pipeline that extracts features from datasets and generates structurally similar, customizable images (scenery, weather, lighting), guided by a text prompt. This data augmentation pipeline is named \emph{Canny2Concrete}. The images generated by the pipeline are already labeled and don't require handcraft labelling.

This technique is then used to construct a novel, large-scale, high-resolution, open-source, dataset of labeled runway images that covers image variants in weather, lighting, background, and occlusion. This dataset is named \emph{GARD: Gustavo's Awesome Runway Dataset}.

This approach greatly increases runway image availability for research, and, by virtue of being open-source, allow other researchers to generate and augment their own synthetic datasets with their own desired characteristics.

This new dataset is evaluated theoretically via similarity metrics like Fr√©chet Inception Distance (FID) and Structural Similarity Index (SSIM), and practically by training detection and segmentation models and comparing performance of the model when training it with an existing dataset.
