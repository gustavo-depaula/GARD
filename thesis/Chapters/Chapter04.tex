\chapter{Implementation}

% \documentclass[12pt]{article}

% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage{lmodern}        % For improved text clarity
% \usepackage{geometry}       % For page layout
% \usepackage{hyperref}       % For hyperlinks
% \usepackage{graphicx}       % For figures/images
% \usepackage{listings}       % For code listings
% \usepackage{amsmath,amssymb}

% % Set up the listings environment for Python

% \begin{document}

% \section{Implementation}

\section{Pipeline}

The pipeline is implemented through four Jupyter notebooks, one for each module.
By convention, the file names are prefixed with \texttt{p\_} to indicate they are part of the pipeline.

\subsection{Template image selection module}

The template image selection module contains an adapter function that reads images from the public LARD dataset \cite{ducoffe_lard_2023}, generates JSON label files, and places the image-label file pairs into the output directory. 
The LARD dataset is composed of several different folders containing both synthetic and real images. 
It has two real images folders: \emph{Nominal cases} and \emph{Edge cases}, the latter being composed of images with poor runway visibility.

Because the template images are used to extract canny edges, it is better to use images with clear runway and surrounding area visibility. 
Hence, the \emph{Nominal cases} folder, containing 1500 images, was chosen.

The images are then processed into square format, since Stable Diffusion by
default works with square images. 
The runway is horizontally centered so it will not be out of frame when cropping into a square. 
The following code does this:

\begin{lstlisting}[language=Python, caption={Template image selection module,
image pre-processing}]
xs = [p[0] for p in pts]
ys = [p[1] for p in pts]
min_x, max_x = min(xs), max(xs)
min_y, max_y = min(ys), max(ys)

# Vertical crop is the entire height
top = 0
bottom = H

# Horizontal positioning for H-wide crop
center_x = (min_x + max_x) / 2.0
ideal_left = int(round(center_x - (H / 2)))
left = max(0, min(ideal_left, W - H))
right = left + H

cropped_img = img.crop((left, top, right, bottom))
\end{lstlisting}

Then, the new shifted runway corners' keypoints are computed to build a JSON label:

\begin{lstlisting}[language=Python, caption={Template image selection module,
label building}]
shifted_pts = [(p[0] - left, p[1] - top) for p in pts]

# Build label
image_label = ImageLabel(
    dataset="LARD/LARD_test_real_nominal",
    sourceImage=os.path.basename(image_path),
    runwayLabel=shifted_pts
)
\end{lstlisting}

Finally, the image label is converted to JSON, and both the cropped images and their labels are saved to the output folder.

\subsection{Edge extraction module}

The edge extraction module contains a generator function that takes an input directory and outputs a directory with corresponding canny edge images at a $1024 \times 1024$ resolution. 
The generator function uses an image processor from a ControlNet Auxiliary library to generate the canny edges, and then resizes to the desired resolution:

\begin{lstlisting}[language=Python, caption={Edge extraction module,
edge extraction}]

from controlnet_aux.processor import Processor

processor = Processor(
    'canny',
    {
        "detect_resolution": 756,
        "image_resolution": 1024
    }
)

pil_in = Image.open(image_path).convert("RGB")
canny_pil = processor(pil_in, to_pil=True).resize((1024, 1024))
\end{lstlisting}

To help in the image generation process, a polygon is drawn onto the image to delimit the area of the runway:

\begin{lstlisting}[language=Python, caption={Edge extraction module,
polygon drawing in canny edge image}]
cv2.polylines(
    canny_array,
    [corners_sorted.astype(np.int32)],
    isClosed=True,
    color=(255, 255, 255),
    thickness=2
)
\end{lstlisting}

Finally, the corners are scaled to this new image resolution and saved to the new label file:

\begin{lstlisting}[language=Python, caption={Edge extraction module,
scaling corners to new resolution}]
orig_h, orig_w = image_bgr.shape[:2]
corners_array = np.array(runway_corners, dtype=np.float32)
corners_array[:, 0] *= (1024.0 / orig_w)
corners_array[:, 1] *= (1024.0 / orig_h)
\end{lstlisting}

\subsection{Base Image generation module}

This is the most important module of the pipeline, as everything depends on the quality of the base images generated in this step. 
To generate the images, the \texttt{diffusers} library
\cite{noauthor_diffusers_nodate} is used, providing utilities for generating
images with pre-trained diffusion models.

The classes one needs from \texttt{diffusers} depend on which model is in use. 
The DreamShaper model \cite{noauthor_dreamshaper_2025} is a general-purpose Stable Diffusion model, meant to compete with other general-purpose models such as DALL-E and Midjourney. 
Of several tested models and pipelines, DreamShaper was chosen for its image quality, faithfulness to the prompt, and ability to construct diverse scenarios (ranging across different lighting and weather conditions).

From the DreamShaper family, we chose the DreamShaper XL version, capable of producing $1024 \times 1024$ images, as it is based on Stable Diffusion XL. 
This higher resolution was necessary to generate finer details such as runway markings.

Having selected DreamShaper XL, it is required to choose a compatible ControlNet
model and scheduler configuration. 
For the ControlNet model, the pre-trained canny-edge-based ControlNet offered by
\texttt{diffusers} for Stable Diffusion XL was used, and the recommended
scheduler configuration for DreamShaper was applied. Enabling CPU offloading was
critical to avoid running out of GPU memory:

\begin{lstlisting}[language=Python, caption={Base Image generation module,
model and scheduler configuration}]
controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16
)

pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(
    "lykon/dreamshaper-xl-v2-turbo",
    torch_dtype=torch.float16,
    variant="fp16",
    controlnet=controlnet
).to("cuda")

pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
    pipeline.scheduler.config,
    algorithm_type="sde-dpmsolver++",
    use_karras_sigmas=True
)

pipeline.enable_model_cpu_offload()
\end{lstlisting}

A critical part of generative models is \emph{prompt engineering}: crafting an input prompt that achieves the desired result. 
Diffusion models accept both a \emph{prompt} and a \emph{negative prompt}. 
The model tries to produce what is mentioned in the prompt and avoid what is mentioned in the negative prompt.

Crafting the prompt is a process of trial and error until satisfactory results are found. 
In practice, it is helpful to look at prompts used in the model's example pages to see keywords the model responds strongly to, such as \texttt{cinematic film still}, \texttt{realistic}, \texttt{ugly}, or \texttt{deformed}.

Since this project needed to create images under diverse scenarios, after trying
many different prompts, the following prompts were chosen for their ability to
generate realistic runway images with different weather and lighting conditions.

Two base prompts were defined for daylight images with no weather adversity, and a dictionary of modifiers was used to add or remove keywords to achieve weather or time-of-day effects:

\begin{lstlisting}[language=Python, caption={Base Image generation module,
prompt definitions}]
base_prompt = "photo of airport runway, aerial view, 4k, cinematic film still, realistic, beautiful landscape around, high-contrast runway lines"

base_neg = "airplane, ugly, low-quality, ugly background, ugly airstrip, deformed, dark, noisy, blurry, low contrast, missing lines, unrealistic, drawing, objects on runway"

modifiers = {
    "rain": (
        "+raining +storm +wet",
        "-dark -noisy -blurry",
    ),
    "fog": (
        "+(harsh fog) +mist +haze",
        "-dark -noisy -blurry",
    ),
    "snow": (
        "+snowing",
        "",
    ),
    "dusk": (
        "+(at dusk)",
        "",
    ),
    "dawn": (
        "+(at dawn)",
        "",
    ),
    "night": (
        "+(at night)",
        "-dark",
    )
}
\end{lstlisting}

A helper function (e.g., \texttt{apply\_modifiers}) builds the final prompts
(and negative prompts) from the base prompts and the modifiers. 
It also receives the number of images to generate, returning a data structure that can be passed to \texttt{generate\_base\_images}, which uses the prompt pairs to generate and save images:

\begin{lstlisting}[language=Python, caption={Base Image generation module,
generating base images}]
generate_base_images(
    "p_FilteredCannyEdges",  # input directory
    "p_BaseImages",          # output directory
    prompt_pairs=[           # images to generate
        apply_modifiers("day", [], 5),
        apply_modifiers("night", ["night"], 5),
        apply_modifiers("dusk", ["dusk"], 1),
        apply_modifiers("dawn", ["dawn"], 1),
        apply_modifiers("fog", ["fog"], 1),
        apply_modifiers("fog+night", ["night", "fog"], 1),
        apply_modifiers("rain", ["rain"], 1),
        apply_modifiers("rain+night", ["night", "rain"], 1),
        apply_modifiers("snow", ["snow"], 1),
        apply_modifiers("snow+night", ["night", "snow"], 1),
    ],
    model_name="sdxl-dreamshaperxl",
    # show=True
)
\end{lstlisting}

\subsection{Variant image generation module}

To generate variant images, three pipelines are run: a positional variant augmentation, outpainting the borders, and weather occlusion effects.

The first pipeline uses the \emph{Albumentations} library \cite{buslaev_albumentations_2020} to run three transformations: 
(1) random horizontal flip, 
(2) padding with a border (to simulate a more distant runway), and 
(3) slight random rotation from $-25^\circ$ to $+25^\circ$:

\begin{lstlisting}[language=Python, caption={Variant image generation module,
Albumentations pipeline}]
pipeline = A.ReplayCompose(
    [
        A.HorizontalFlip(p=0.5),
        A.CropAndPad(
            px=((51, 410),  # top
                (51, 410),  # bottom
                (51, 410),  # left
                (51, 410)), # right
            keep_size=False,
            p=1.0
        ),
        A.Affine(rotate=(-25, 25), p=1.0),
    ],
    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)
)
\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/albumentations.png}
  \caption{Applying the \emph{Albumentations} pipeline to an image.}
\end{figure}

This first pipeline generates images with black borders. 
The second pipeline uses \emph{inpainting} to fill in these black borders. 
First, OpenCV-based inpainting is used, filling the black border with colors from the nearby region. 
Then, a Stable Diffusion model refines the borders for better blending.

OpenCV-based inpainting is used because Stable Diffusion inpainting works
better when the inpainting region is filled with a rough approximation of the
missing content.

OpenCV implements Alexandru Telea's inpainting algorithm \cite{telea_image_2004}
via \texttt{cv2.inpaint} with the \texttt{cv2.INPAINT\_TELEA} flag:

\begin{lstlisting}[language=Python, caption={Variant image generation module,
OpenCV-based inpainting}]
mask = np.all(image == 0, axis=2).astype(np.uint8) * 255
kernel = np.ones((3, 3), np.uint8)
mask = cv2.dilate(mask, kernel, iterations=4)
image = Image.fromarray(cv2.cvtColor(
    cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA),
    cv2.COLOR_BGR2RGB
))
\end{lstlisting}

After this initial inpainting, a Stable Diffusion XL Inpainting pipeline is used to process the image. 
The original prompts used for the base images are reused, along with a blurred mask for smoother blending:

\begin{lstlisting}[language=Python, caption={Variant image generation module,
Stable Diffusion XL Inpainting pipeline}]
pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
    "lykon/dreamshaper-xl-lightning",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

# [...]

mask = pipe.mask_processor.blur(Image.fromarray(mask), blur_factor=75)

result = pipe(
    prompt=data["prompt"],
    negative_prompt=data["negative_prompt"],
    image=image,
    mask_image=mask,
    strength=0.8,
    generator=generator,
    num_inference_steps=30,
    guidance_scale=2,
).images[0]
\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/outpaint.png}
  \caption{Applying the outpainting pipeline to an image.}
\end{figure}

The third pipeline reads the images output by the second pipeline and creates a new folder with the same images, further augmented with weather occlusion effects done by \texttt{imgaug} \cite{jung_imgaug_2020}. 
Effects are chosen depending on the variant:

\begin{itemize}
\item Clouds for \emph{pure} \texttt{day/night/dusk/dawn} images
\item Fog for \texttt{fog} images
\item Rain for \texttt{rain} images
\item Snowflakes for \texttt{snow} images
\end{itemize}

For example:

\begin{lstlisting}[language=Python, caption={Variant image generation module,
\texttt{imgaug} pipeline}]
if variant in ["day", "night", "dusk", "dawn"]:
    aug = iaa.SomeOf((1, 2), [
        iaa.CloudLayer(...),
        iaa.CloudLayer(...),
    ])
    image = aug(image=image)
    applied_effects.append("clouds")
elif variant in ["fog", "fog+night"]:
    aug = iaa.CloudLayer(...)
    image = aug(image=image)
    applied_effects.append("fog")
elif variant in ["rain", "rain+night"]:
    aug = iaa.Rain(drop_size=(0.1, 0.2), speed=(0.01, 0.05))
    image = aug(image=image)
    applied_effects.append("light_rain")
elif variant in ["snow", "snow+night"]:
    aug = iaa.Snowflakes(flake_size=(0.1, 0.3), speed=(0.01, 0.05))
    image = aug(image=image)
    applied_effects.append("snowflakes")
\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/occlusion.png}
  \caption{Applying the occlusion pipeline to an image.}
\end{figure}


After each augmentation pipeline, the label JSON file is enriched with data to ensure reproducibility, including random seeds and applied transformations.

\section{Filtering Tool}

A manual filtering tool was implemented in Python with OpenCV to assist in selecting template images. 
It reads images from a directory and allows pressing \texttt{Space} to select an image or \texttt{X} to discard. 
Selected images are copied into a new directory, and a log file retains progress so the tool can be closed and resumed later without starting from scratch.

After a first test running the Canny2Concrete pipeline, it was clear that not
all canny edges generated from the nominal dataset were suitable for generating
realistic runway images. Thus, to filter which images would be used as template
images, the filtering tool was used to select only the images that effectively
generated a realistic runway image.

Template images were filtered as follows:
\begin{enumerate}
\item Use all images from \texttt{LARD\_test\_real\_nominal} as template images, pass them through the canny-edge and base-image generation modules, generating one daylight base image per template.
\item Use the filtering tool to select which base images had easily recognizable runways with consistent markings and structure.
\item Then, only use these selected images as template images to generate the datasets.
\end{enumerate}

\section{Datasets generation}

Three datasets are generated with the Canny2Concrete pipeline:

\begin{enumerate}
\item \texttt{BaseImages}: 18 variations for each of the canny edge images.
\item \texttt{VariantImages}: 3 variations for each base image \emph{before} weather occlusion.
\item \texttt{VariantImagesWithOcclusion}: same as \texttt{VariantImages} but with weather effects applied.
\end{enumerate}

Each image is associated with four files:
\begin{enumerate}
\item \texttt{.png} file (the image itself).
\item \texttt{.json} file (the image label).
\item \texttt{.mask.png} file (a segmentation mask: black background, white runway).
\item \texttt{.txt} file (a YOLO-format label for detection/segmentation training).
\end{enumerate}

The JSON label file carries the runway keypoints as well as all data required for reproducing that specific image, including prompts, seeds, variant details, transformations, and so on. 
This metadata supports image classification (by variant or weather type), label fixes without regenerating images, and easier reproducibility/peer review.

Each dataset also includes \texttt{train.txt}, \texttt{test.txt}, and \texttt{val.txt} splits in 80/10/10 proportions, ensuring images sharing the same source canny edge (and thus similar structure) do not leak from training into testing.

These three datasets are the parts of \emph{GARD: Gustavo's Awesome Runway Dataset}.

\section{Evaluation}

The evaluation is done in two parts: intrinsic and extrinsic, through five
Jupyter notebooks. By convention, the file names are prefixed with \texttt{r\_} to
indicate they are part of the results' evaluation.

\subsection{Intrinsic evaluation}

For the intrinsic evaluation, \ac{SSIM} is used to compare the generated images
with their original templates. To compute the SSIM, the \texttt{pytorch-msssim}
\cite{pessoa_jorge-pessoapytorch-msssim_2025}
was used instead of the more commonly used \texttt{scikit-image}
\cite{van_der_walt_scikit-image_2014}
implementation,
as the former runs on the GPU and is faster.

This function groups SSIM scores by variant type (e.g., \texttt{day},
\texttt{fog+night}) for later statistical analysis and comparison:

\begin{lstlisting}[language=Python, caption={Evaluation, SSIM score calculation}]
def calculate_ssim_values(folder):
    image_files = [f for f in os.listdir(folder) if f.lower().endswith(".png") and not f.lower().endswith(".mask.png")]

    results = {}
    
    for img_name in tqdm(image_files, desc="Processing images", unit="img"):
        ...

        if not results.get(variant):
            results[variant] = []

        template = cv2.imread(f'p_Templates/{sourceImage}')
        template.resize((1024,1024,3))

        image_tensor = torch.from_numpy(image)
                            .float().permute(2, 0, 1)
                            .unsqueeze(0).to(device) / 255.0
        template_tensor = torch.from_numpy(template)
                               .float().permute(2, 0, 1)
                               .unsqueeze(0).to(device) / 255.0

        with torch.no_grad():
            score = ssim(
                template_tensor,
                image_tensor,
                data_range=1.0,
                size_average=True
            ).item()

        results[variant].append(score)

    return results
\end{lstlisting}

\subsection{Experimental evaluation}

Experimental evaluation was done by training and fine-tuning YOLOv11 models on
the generated datasets and on the LARD dataset, serving as a baseline. The YOLO
architecture was chosen because it is commonly used for object detection and
segmentation tasks, was widely used in the literature review, and has strong community
support. YOLOv11 was chosen because it is the latest and best-performing YOLO model.

\subsubsection{Generating Labels and Masks for GARD}

Before training the YOLO model, it is necessary to create labels in the YOLO
format. To make it also easier to train other segmentation models, a binary mask
is also generated for each image. Both the YOLO TXT labels and the masks are
published alongside the images.

\begin{lstlisting}[language=Python, caption={Evaluation, generating labels and masks}]
def polygon_to_yolo_label_from_image_dict(image: Image.Image, data: dict, class_id=0) -> str:
    """
    Converts the 'runwayLabel' polygon from the data dict into YOLO segmentation format:
    <class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>
    
    Coordinates are normalized to [0, 1] based on the image dimensions.
    """
    if "runwayLabel" not in data:
        raise KeyError("'runwayLabel' key not found in data dict.")
    
    polygon = ensure_clockwise_quad(data["runwayLabel"])
    width, height = image.size

    # Normalize coordinates to 0-1 range
    normalized_points = [(pt[0] / width, pt[1] / height) for pt in polygon]
    
    flat_coords = " ".join(f"{x:.6f} {y:.6f}" for x, y in normalized_points)
    return f"{class_id} {flat_coords}"


def create_runway_segmentation_mask(image: Image.Image, data: dict) -> Image.Image:
    if "runwayLabel" not in data:
        raise KeyError("'runwayLabel' key not found in data dict.")
    
    polygon = ensure_clockwise_quad(data["runwayLabel"])
    width, height = image.size
    mask = Image.new("L", (width, height), 0)
    draw = ImageDraw.Draw(mask)
    draw.polygon([(int(x), int(y)) for (x, y) in polygon], fill=255)

    return mask
\end{lstlisting}


\subsubsection{Assembling YOLO datasets}

YOLO requires a very specific dataset folder structure to fine-tune and validate
the model. To convert the GARD and LARD datasets into these formats, symlinks
were used to avoid duplicating the images and labels.

First, splits are created for the GARD datasets:

\begin{lstlisting}[language=Python, caption={Evaluation, GARD train/test/val splits}]
def create_yolo_dataset(folder_path: str):
    """
    Creates train.txt, test.txt, val.txt, and dataset.yaml inside `folder_path`
    based on PNG images in `folder_path` (excluding *.mask.png files).
    
    Splits data by original_id (to avoid data leakage) into
    80% train, 10% test, 10% val.
    
    NOTE: The paths inside train.txt/test.txt/val.txt will be relative
    to `folder_path`.
    """

    n = len(original_ids)
    train_end = int(0.80 * n)
    test_end  = int(0.90 * n)

    train_ids = original_ids[:train_end]
    test_ids  = original_ids[train_end:test_end]
    val_ids   = original_ids[test_end:]

    ...
\end{lstlisting}

As the comments indicate, the splits are done by the \texttt{sourceImage}
field, which references the original template image. This ensures that images
generated from the same template are not split across train/test/val, avoiding
data leakage.

Then, the following code assembles the dataset:

\begin{lstlisting}[language=Python, caption={Evaluation, assembling GARD dataset
in YOLO structure}]
def process_folder_structure_with_labels(folder_path: str):
    """
    Reads train.txt, test.txt, val.txt from `folder_path` (which was created by create_yolo_dataset).
    Builds a YOLO directory structure in `datasets/<folder_name>`, placing:
    
      datasets/<folder_name>/
        images/train/ -> symlinks to the train images
        images/test/  -> symlinks to the test images
        images/val/   -> symlinks to the val images
        labels/train/ -> symlinks to the corresponding train labels
        labels/test/  -> symlinks to the corresponding test labels
        labels/val/   -> symlinks to the corresponding val labels
        dataset.yaml  -> references these subfolders

    :param folder_path: The path to your original folder (e.g. "p_BaseImages"),
                        which must have train.txt, test.txt, val.txt, dataset.yaml
                        already generated by create_yolo_dataset().
    """

    src_dir = Path(folder_path).resolve()
    folder_name = src_dir.name  # e.g. "p_BaseImages"

    # The new dataset directory, e.g. datasets/p_BaseImages
    dst_base = Path("datasets") / folder_name
    dst_base.mkdir(parents=True, exist_ok=True)

    # keep the standard YOLO layout:
    # images/train, images/test, images/val, plus labels/train, labels/test, labels/val
    images_dir = dst_base / "images"
    labels_dir = dst_base / "labels"
    train_img_dir = images_dir / "train"
    test_img_dir  = images_dir / "test"
    val_img_dir   = images_dir / "val"
    train_lbl_dir = labels_dir / "train"
    test_lbl_dir  = labels_dir / "test"
    val_lbl_dir   = labels_dir / "val"

    train_img_dir.mkdir(parents=True, exist_ok=True)
    test_img_dir.mkdir(parents=True, exist_ok=True)
    val_img_dir.mkdir(parents=True, exist_ok=True)
    train_lbl_dir.mkdir(parents=True, exist_ok=True)
    test_lbl_dir.mkdir(parents=True, exist_ok=True)
    val_lbl_dir.mkdir(parents=True, exist_ok=True)

    train_file = src_dir / "train.txt"
    test_file  = src_dir / "test.txt"
    val_file   = src_dir / "val.txt"

    ...
    # Symlink the train, test, val sets
    symlink_split(train_file, train_img_dir, train_lbl_dir)
    symlink_split(test_file,  test_img_dir,  test_lbl_dir)
    symlink_split(val_file,   val_img_dir,   val_lbl_dir)

    ...
    dst_dataset_yaml = dst_base / "dataset.yaml"
    with dst_dataset_yaml.open("w") as f:
        f.write(dataset_yaml_content)
\end{lstlisting}

Similar functions were used to assemble the LARD dataset in YOLO format, and
also the LARD test real datasets, which were used for validation.

\subsubsection{Fine-tuning YOLOv11}

Once the dataset is assembled in YOLO's format, it is very easy to fine-tune a
pre-trained model:

\begin{lstlisting}[language=Python, caption={Evaluation, fine-tuning YOLOv11}]
from ultralytics import YOLO

model = YOLO("yolo11n-seg.pt")
results = model.train(
    data="./datasets/p_BaseImages/dataset.yaml",
    epochs=100,
    patience=10,
    project="trained_models/yolo",
    name="gard-BaseImages",
)
\end{lstlisting}

Epochs and patience are hyperparameters that can be adjusted, with epochs
indicating the maximum number of training epochs and patience indicating the
number of epochs to wait before early stopping if the validation loss does not
improve. YOLO saves the best and latest weights. These weights are also
published on the project's Kaggle repository.

\subsubsection{Validation}

Using the LARD test real datasets, the trained models are validated using
YOLO's own validation function:

\begin{lstlisting}[language=Python, caption={Evaluation, YOLOv11 validation}]
import os
from ultralytics import YOLO

def evaluate_all_models():
    models_base_path = "trained_models/yolo"
    properties = {
        "save_json": True,
        "plots": True,
        "imgsz": 640,
        "batch": 16,
        "conf": 0.50,
        "iou": 0.6,
        "device": "0",
        "save_txt": True,
        "save_conf": True,
        "save_crop": True
    }
    
    results_dict = {}
    
    for model_dir in os.listdir(models_base_path):
        ...
        
        model = YOLO(best_weights_path)

        nominal_results = model.val(
            data="./datasets/LARD-real-nominal/dataset.yaml",
            **properties,
        )
        edge_cases_results = model.val(
            data="./datasets/LARD-real-edge-cases/dataset.yaml",
            **properties,
        )
        
        results_dict[model_dir] = {
            "nominal": nominal_results,
            "edge_cases": edge_cases_results
        }

    return results_dict
\end{lstlisting}

The results are both saved to folders that are published on the project's Kaggle
repository and returned as a dictionary for easier analysis in the Jupyter
notebook itself.
